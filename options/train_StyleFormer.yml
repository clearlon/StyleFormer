#### general settings
name: StyleFormerNet-001-enc16-m2-dec4-embedding32-dp05-b16-L1loss
model_type: IFAModel
scale: 1
num_gpu: 1  # set num_gpu: 0 for cpu mode
manual_seed: 0

#### datasets
datasets:
  train:
    name: IFFI
    type: IFFIDataset #IFFIDatasetFast
    dataroot: datasets/IFFI/IFFI-dataset-lr-train
    data_range: 255.
    guidedFilter: true

    use_flip: true
    use_rot: true

    # data loader
    use_shuffle: true
    num_worker_per_gpu: 8
    batch_size_per_gpu: 2
    pin_memory: false
    dataset_enlarge_ratio: 1
    prefetch_mode: ~

#### network structures
network_g:
  type: StyleFormerNet
  img_channel: 3
  embedding: 32
  width: 32
  enc_blk_nums: [1,1,7,7]
  middle_blk_num: 2
  dec_blk_nums: [1,1,1,1]
  drop_type: 'dropout'
  drop_out_rate: 0.5
  load_path: 'experiments/pretrained/resnetarcface.pth'

#### path
path:
  pretrain_network_g: ~
  strict_load_g: true
  resume_state: ~

#### training settings: learning rate scheme, loss
train:
  # ema_decay: 0.999
  optim_g:
    type: Adam
    lr: !!float 5e-4
    weight_decay: !!float 1e-8
    betas: [0.9, 0.999]

  scheduler:
    type: MultiStepLR
    milestones: [200000, 300000, 350000, 500000]
    gamma: 0.5

  use_grad_clip: true
  total_iter: 600000
  warmup_iter: -1  # no warm up
  fix_flow: 5000
  flow_lr_mul: 0.125

  # losses
  pixel_opt:
    type: L1Loss #CharbonnierLoss
    loss_weight: 1.0
    reduction: mean

# # logging settings
logger:
  print_freq: 10
  save_checkpoint_freq: !!float 1e4
  use_tb_logger: true
  wandb:
    project: ~
    resume_id: ~
